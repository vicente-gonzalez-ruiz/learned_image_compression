{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicente-gonzalez-ruiz/learned_image_compression/blob/main/notebooks/07__one_Conv2Dlayer_AE_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nEXw-Gz5GV6"
      },
      "source": [
        "# Learned Image Compression (LIC) using [(Compressive) Autoencoders](https://www.lherranz.org/2022/08/24/neural-image-compression-in-a-nutshell-part-1-main-idea/) using 1 hidden layer and no quantization of the latent space\n",
        "\n",
        "       input                           output\n",
        "    +---------+                      +---------+\n",
        "    | 28x28x1 |                      | 28x28x1 |\n",
        "    +---------+                      +---------+\n",
        "         | flatten()                      ^ reshape(28, 28, 1)\n",
        "         v                                |\n",
        "     +-------+ dense() +----+ dense() +-------+\n",
        "     | 28*28 |-------->| 32 |-------->| 28*28 |\n",
        "     +-------+         +----+         +-------+\n",
        "                     code layer\n",
        "    <----------------------->\n",
        "           encoder             decoder\n",
        "                       <---------------------->"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that we have found a lossy compact representation of 28x28 scalars (integers) using only 32 (floats) scalars."
      ],
      "metadata": {
        "id": "09rtLkc_76Et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A simple autoencoder with only one hidden layer.\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.utils import Sequence\n",
        "from skimage.io import imread\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage.transform import resize\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "worBuXiCX0S0",
        "outputId": "ee087047-c7af-47ff-9e45-35b0eef9deba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This variable controls the number of hidden neurons. Notice that the output of a neuron is a floating point number.\n",
        "LATENT_SPACE_LENGTH = 32\n",
        "\n",
        "img_width, img_height = 28, 28\n",
        "img_length = img_height * img_width\n",
        "\n",
        "DATASET = 'mnist'  # See https://www.tensorflow.org/datasets/catalog/\n",
        "DATA_URL = f'https://storage.googleapis.com/tensorflow/tf-keras-datasets/{DATASET}.npz'\n",
        "# DATASET = 'fashion_mnist'\n",
        "# DATA_URL = \"https://github.com/zalandoresearch/fashion-mnist/blob/master/data/fashion/train-images-idx3-ubyte.gz\"\n",
        "path = tf.keras.utils.get_file(f'{DATASET}.npz', DATA_URL)\n",
        "#with np.load(path) as data:\n",
        "#    x_train = data['x_train']\n",
        "#    y_train = data['x_train']\n",
        "#    x_test = data['x_test']\n",
        "#    y_test = data['x_test']\n",
        "with np.load(path) as data:\n",
        "#    x_train = data['x_train']\n",
        "#    x_test = data['x_test']\n",
        "#    y_train = x_train\n",
        "#    y_test = x_test\n",
        "    x_train = data['x_train']\n",
        "    y_train = data['x_train']\n",
        "    x_test = data['x_test']\n",
        "    y_test = data['y_test']\n",
        "\n",
        "# Pipelines.\n",
        "train_DS = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "test_DS = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "\n",
        "# Define how to use the datasets, preprocess, etc.\n",
        "\n",
        "def process_x(image, label):\n",
        "    # The input images are reshaped (to have only one dimension) and normalized to [0, 1].\n",
        "    processed_img = tf.reshape(image, (img_height * img_width, 1))\n",
        "    processed_img = tf.cast(processed_img, tf.float32) / 255.\n",
        "    # The input dataset has been labeled, but labels are ignored in the rest of the pipeline.\n",
        "    return processed_img, label\n",
        "\n",
        "def process_y(image, label):\n",
        "    # The output images are already in 1D. Notice that in autoencoder, the objective is to generate the same output than the input.\n",
        "    return image, image\n",
        "\n",
        "# During each training iteration, the model will process batches of 64 examples, and the training data will be shuffled with a buffer size of 100.\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE_BUFFER_SIZE = 100\n",
        "train_DS = train_DS.map(process_x)\n",
        "train_DS = train_DS.map(process_y)\n",
        "train_DS = train_DS.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "test_DS = test_DS.map(process_x)\n",
        "test_DS = test_DS.map(process_y)\n",
        "test_DS = test_DS.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "g7JKg0uNo3ZP",
        "outputId": "3997a570-82f9-455d-9623-8f02aac90356",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A autoencoder definition (you can modify, for example, the activation fuctions).\n",
        "num_channels = 1\n",
        "input_layer = keras.Input(shape=(img_height, img_width, num_channels))\n",
        "conv2d_layer = layers.Conv2D(20, 5, use_bias=True, strides=2, padding=\"same\", activation=\"leaky_relu\")(input_layer)\n",
        "hidden_layer = layers.Dense(LATENT_SPACE_LENGTH, activation=\"relu\")(conv2d_layer)\n",
        "conv2d_trans_layer = layers.Conv2DTranspose(1, 5, use_bias=True, strides=2, padding=\"same\", activation=\"leaky_relu\")(hidden_layer)\n",
        "output_layer = layers.Dense(img_length, activation=\"sigmoid\")(conv2d_trans_layer)"
      ],
      "metadata": {
        "id": "4YMbgkf2pDD9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder = keras.Model(input_layer, output_layer)"
      ],
      "metadata": {
        "id": "RjqbmS1nrgvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To see the \"latents\" (content of the hidden layer) while we train the network, we define a new \"model\" with only the encoder.\n",
        "encoder = keras.Model(input_layer, conv2d_layer, hidden_layer)"
      ],
      "metadata": {
        "id": "0T10KeTlpK_n"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=\"mse\")\n",
        "autoencoder.summary()"
      ],
      "metadata": {
        "id": "05KGCTsAqsTq",
        "outputId": "5cfb8ede-c8f3-49b9-e727-2567901a4acc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 14, 14, 20)        520       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 14, 14, 32)        672       \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2D  (None, 28, 28, 1)         801       \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 28, 28, 784)       1568      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3561 (13.91 KB)\n",
            "Trainable params: 3561 (13.91 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's train.\n",
        "\n",
        "# Number of times that the complete dataset is used to train the autoencoder. In this case, because we want to show the progressive reconstruction of some images, we train the encoder using only one epoch.\n",
        "EPOCHS = 1\n",
        "\n",
        "# Number of times that the autoencoder is trained. Notice that if we don't want to see how the model is learning between epochs, it is not necessary to iterate the fit() method, because we can increase the number of epochs.\n",
        "ITERATIONS = 3\n",
        "plt.gray()\n",
        "for i in range(ITERATIONS):\n",
        "    autoencoder.fit(train_DS, epochs=EPOCHS, validation_data=test_DS)\n",
        "\n",
        "    # Show the learning.\n",
        "    plt.figure(figsize=(40, 4))\n",
        "\n",
        "    n = len(train_DS)\n",
        "    if n > 32:\n",
        "        n = 32\n",
        "\n",
        "    # Show the originals (using the pipeline).\n",
        "    for images in test_DS.take(1):\n",
        "        for i in range(n):\n",
        "            ax = plt.subplot(3, n, i + 1)\n",
        "            try:\n",
        "                image = images[0][i].numpy()\n",
        "                image *= 255\n",
        "                image = image.astype(\"uint8\").reshape(img_height, img_width)\n",
        "                plt.imshow(image)\n",
        "                plt.axis(\"off\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    # See the latent space (as images).\n",
        "    latent_imgs = encoder.predict(test_DS)\n",
        "    #print(\"type latent\", type(latent_imgs))\n",
        "    for i in range(n):\n",
        "        ax = plt.subplot(3, n, i + 1 + n)\n",
        "        plt.imshow(resize(latent_imgs[i], (LATENT_SPACE_LENGTH, img_width)))\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    # See the reconstructions.\n",
        "    reconstructed_imgs = autoencoder.predict(test_DS)\n",
        "    #print(type(reconstructed_imgs))\n",
        "    for i in range(n):\n",
        "        ax = plt.subplot(3, n, i + 1 + 2 * n)\n",
        "        plt.imshow(reconstructed_imgs[i].reshape(img_height, img_width))\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "MpoFmmNpqxHU",
        "outputId": "7c0a67b9-e888-4ef5-d424-7c84095b5219",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-4c5d487d3521>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mITERATIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_DS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_DS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Show the learning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_2\" is incompatible with the layer: expected shape=(None, 28, 28, 1), found shape=(None, 784, 1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eSLkFSeDrLDe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "07__one_Conv2Dlayer_AE_MNIST.ipynb",
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}