<!DOCTYPE html> 
<html lang='en-US' xml:lang='en-US'> 
<head> <title>Image Compression with (Compressive) Autoencoders</title> 
<meta charset='utf-8' /> 
<meta content='TeX4ht (https://tug.org/tex4ht/)' name='generator' /> 
<meta content='width=device-width,initial-scale=1' name='viewport' /> 
<link href='index.css' rel='stylesheet' type='text/css' /> 
<meta content='index.tex' name='src' /> 
<script>window.MathJax = { tex: { tags: "ams", }, }; </script> 
 <script async='async' id='MathJax-script' src='https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js' type='text/javascript'></script>  
</head><body>
   <div class='maketitle'>
                                                                  

                                                                  
                                                                  

                                                                  

<h2 class='titleHead'>Image Compression with (Compressive)
Autoencoders</h2>
 <div class='author'><a href='https://cms.ual.es/UAL/personas/persona.htm?id=515256515553484875'><span class='ecrm-1200'>Vicente González Ruiz</span></a> <span class='ecrm-1200'>- </span><a href='https://cms.ual.es/UAL/universidad/departamentos/informatica/index.htm'><span class='ecrm-1200'>Depto Informática</span></a> <span class='ecrm-1200'>- </span><a href='https://www.ual.es'><span class='ecrm-1200'>UAL</span></a></div><br />
<div class='date'><span class='ecrm-1200'>December 2, 2023</span></div>
   </div>
<!-- l. 5 --><p class='indent'>   An <a href='https://www.lherranz.org/2022/08/24/neural-image-compression-in-a-nutshell-part-1-main-idea/'>autoencoder</a> consists of two main components: the encoder and the decoder.
The encoder is a function \begin {equation}  e(\mathbf {X})=\mathbf {Z},  \end {equation}
that maps the input data \(\mathbf {X}\) to the encoded representation in the latent space \(\mathbf {Z}\). The
decoder is the inverse function \begin {equation}  d(\mathbf {Z})=(\mathbf {X}),  \end {equation}
that maps the encoded representation back to the original input space. When an
autoencoder is implemented using <a href='https://en.wikipedia.org/wiki/Artificial_neural_network'>ANNs</a> (with parameters \(\psi =(\theta ,\phi )\), where \(\theta \) are the
parameters of the encoder, and \(\phi \) are the parameters of the decoder), and the
dimension (the number of elements) of the latent space (the output of the
smaller hidden layer) is smaller than the dimension of the input (and output)
space, it is impossible (in most of the cases) to decode the original \(\mathbf {X}\), but
only one an approximation \(\hat {\mathbf {X}}(\psi )\). For this reason, the cost function \(J\) optimized
(minimized) in an autoencoder is usually the MSE, defined in this case as
\begin {equation}  D(\mathbf {X}, \hat {\mathbf {X}})=\frac {1}{N}\sum _{i}^{N}(\mathbf {X}_i-\hat {\mathbf {X}}_i)^2,  \end {equation}
where \(N\) is the dimension of \(\mathbf {X}\) (and \(\hat {\mathbf {X}}\)), and \(\mathbf {X}_i\) is the \(i\)-th element of \(\mathbf {X}\).
</p><!-- l. 35 --><p class='indent'>   Therefore, we can formally define an autoencoder as finding the optimal
autoencoder parameters \begin {equation}  \psi ^*=\min _\psi D(\mathbf {X}, \hat {\mathbf {X}}(\psi )).  \end {equation}
</p><!-- l. 41 --><p class='indent'>   In a compressive autoencoder, we also take into consideration the number of bits \(R(\mathbf {Z})\)
required to represent \(\mathbf {Z}\), that we want also to minimize. Therefore, in this case the cost
function is defined as the Lagrangian \begin {equation}  J(\mathbf {X}, \hat {\mathbf {X}}(\psi ); \lambda )=D(\mathbf {X}, \hat {\mathbf {X}}(\psi )) + \lambda R(\mathbf {Z}),  \end {equation}
where \(\lambda \) is a hyper-parameter, and we optimize \begin {equation}  \psi ^*=\min _\psi J(\mathbf {X}, \hat {\mathbf {X}}(\psi ); \lambda ).  \end {equation}
</p><!-- l. 53 --><p class='indent'>   \(\lambda \) determines the trading-off between rate \(R\) and distortion \(D\). If \(\lambda \) is big we are
priorizing (minimizing) \(R\) and therefore, we will get a point in the RD-curve with a
small bit-rate (high MSE). On the contrary, if \(\lambda \) is small we are minimizing \(D\) and
therefore, we will be in a point of the RD-curve with a high bit-rate (small
MSE).
</p><!-- l. 60 --><p class='indent'>   \(\mathbf {Z}\) is usually compressed with an entropy encoder (notice that this encoder is
                                                                  

                                                                  
different to the "latent encoder" \(e\)). When the code-words assigned to the symbols \(\mathbf {Z}_i\)
depends on the probability distribution of \(\mathbf {Z}\), such probabilities \(p_\psi (\mathbf {Z})\) can be also optimized,
where now \(\psi =(\theta ,\phi , \nu )\).
</p><!-- l. 67 --><p class='indent'>   Finally, notice that we are not constrained to use the MSE as the distortion
metric. Other metrics such as the SSIM (more representative of how humans perceive
the differences between two images) could be also used.
<a id='Q1-1-1'></a>
</p>
    
</body> 
</html>